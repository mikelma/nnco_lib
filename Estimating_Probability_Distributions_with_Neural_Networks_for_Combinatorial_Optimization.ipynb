{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Estimating Probability Distributions with Neural Networks for Combinatorial Optimization\n",
        "\n",
        "This notebook shows two simple examples of the python library developed for the *Estimating Probability Distributions with Neural Networks for Combinatorial Optimization* paper.\n",
        "\n",
        "The repository of the project is available in [SourceHut](https://git.sr.ht/~mikelma/nnco_lib) and in [GitHub](https://github.com/mikelma/nnco_lib) (mirror).\n",
        "\n",
        "\n",
        "\n",
        "> **Quick note:**  The objective of this document is to illustrate practical usage of the ideas developed in the paper, and it assumes that the user has read it. Feel free to [send me a mail](mailto:mmalagon002@ikasle.ehu.eus) if you have any question!"
      ],
      "metadata": {
        "id": "QVD4NSuop89j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dependencies\n",
        "\n",
        "First of all, let's clone the project's repository and build it's dependencies. This might take few minutes to complete, as some dependencies have to be compiled from source.\n",
        "\n",
        "> If you are running this notebook in your own machine, please make sure that you have  \n",
        "[Rust](https://www.rust-lang.org/learn/get-started) (version 1.41 and up) and [PyTorch](https://pytorch.org/get-started/locally/) (tested in v1.12) installed.\n",
        "\n",
        "*The nnco_lib repository provides:*\n",
        "- A python library implementing the optimization framework proposed in the paper.\n",
        "- The set of optimization instances used in the paper (LOP and PFSP).\n",
        "- The `pypermu` python library: provides fast implementations of common permutation operations and problems (it is implemented using the Rust programming language)."
      ],
      "metadata": {
        "id": "A0XUluHRq2_9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "avSzqFnWllJY",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Dependency installation & building\n",
        "# install build dependencies\n",
        "!apt install rust-all\n",
        "# clone project's repo\n",
        "!git clone --recurse-submodules https://git.sr.ht/~mikelma/nnco_lib\n",
        "# change notebook's cwd\n",
        "%cd nnco_lib\n",
        "# build pypermu libarary\n",
        "!cd pypermu && cargo b --release && mv target/release/libpypermu.so ../pypermu.so\n",
        "# unzip LOP instances\n",
        "!mkdir -p all_instances/{LOP,PFSP} && cd all_instances/LOP && unzip -q ../../instances/LOP/IO.zip && cd ../PFSP && unzip -q ../../instances/PFSP/PFSP.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Select correct git branch\n",
        "import sys,os\n",
        "# get runtime's python version\n",
        "py_version = int(sys.version.split(' ')[0].split('.')[1])\n",
        "# check if the minor version is below 9\n",
        "if py_version <= 8:\n",
        "    # move to the `cluster` branch. This branch has few modifications \n",
        "    # in the typing that makes it work with python's below v3.9 \n",
        "    print(\"* Python version lower than 3.9: moving to `cluster` branch\")\n",
        "    os.system(\"git checkout cluster\")"
      ],
      "metadata": {
        "id": "sV9wQuJlQe5j",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optimization of a LOP instance\n",
        "\n",
        "In this section we will use the optimization scheme introduced in the paper to approach a LOP instance.\n",
        "\n",
        "The code next block loads all the necessary modules and data needed to run this example. Note that the only python requirements are [PyTorch](https://pytorch.org/) and [`nnco_lib`](https://git.sr.ht/~mikelma/nnco_lib) (where the paper's ideas have been implemented)."
      ],
      "metadata": {
        "id": "a_XvT-wWrThg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.optim import Adam\n",
        "import torch.nn as nn\n",
        "\n",
        "# libraries included in the `nnco_lib`  repository \n",
        "from nnco.pl import PLHead\n",
        "from nnco import utility\n",
        "from pypermu import problems"
      ],
      "metadata": {
        "id": "JdesS2OspMVh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next code block is used to detect if there's a GPU card available to use. GPU cards highly increase the optimization speed of this algorithm, however, for the simple examples provided in this notebook a GPU card isn't mandatory."
      ],
      "metadata": {
        "id": "h00xr3mB_77_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Check for available GPU \n",
        "\n",
        "if torch.cuda.device_count() > 0:\n",
        "    device = 'cuda:0'\n",
        "else:\n",
        "    device = 'cpu'\n",
        "\n",
        "print(f\"* Device: {device}\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "r3Fn-4Ig_f42"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's select the path where the LOP instance to optimize is located. In this code block the size of the instance has also been specified for later usage."
      ],
      "metadata": {
        "id": "IQ4MQWvTYTSr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "instance = \"all_instances/LOP/N-be75eec\"\n",
        "size = 50\n",
        "\n",
        "problem = problems.lop.Lop(instance)"
      ],
      "metadata": {
        "id": "hDzqU9_gYTgO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the following cell the hyperparameters of the algorithm are defined. \n",
        "The provided parameters are some sane defaults, but feel free to play with this values!"
      ],
      "metadata": {
        "id": "Z3YvwpRLaKdI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of solutions sampled per iteration from the probability distribution \n",
        "# over the solution space of the problem.\n",
        "num_samples = 64\n",
        "\n",
        "# Dimension of the noise vector: the input to the generative model.\n",
        "noise_dim = 128\n",
        "\n",
        "# Same as the usual batch size and learning rate in ML.\n",
        "batch_size = 32\n",
        "learning_rate = 0.003"
      ],
      "metadata": {
        "id": "SxJz-c7ebNYS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As the algorithm's execution has to be somehow limited, in this example we'll follow the same stopping criterion as in the paper: the number of solutions that the algorithm is able to evaluate is limited. \n",
        "<br> <br>\n",
        "\n",
        "Following the paper,\n",
        "$$\n",
        "\\text{max_evals} = 1000n^2\n",
        "$$\n",
        "\n",
        "<br> <br>\n",
        "Consequently,\n",
        "$$\n",
        "\\text{max_iters} = \\frac{\\text{max_evals}}{\\text{batch_size} * \\text{num_samples}} \n",
        "$$\n"
      ],
      "metadata": {
        "id": "DAK9m_dCbjrZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_evals = 1000*size**2\n",
        "num_iters = int(max_evals/(batch_size*num_samples))\n",
        "\n",
        "print(\"* Maximum solution evaluations:\", max_evals)\n",
        "print(\"* Total number of iterations:  \", num_iters)"
      ],
      "metadata": {
        "id": "jLMJv_xldDyD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining the generative model \n",
        "\n",
        "In this code block, the generative model is defined as a PyTorch model. \n",
        "Again, feel free to play with different NN architectures. However, be sure to always set the input size equal to the dimension of the noise vector (`noise_dim`) and the ouput shape to: `num_samples` $\\times$ `sample_size`.\n",
        "\n",
        "> The last layer of the model, [`PLHead`](https://git.sr.ht/~mikelma/nnco_lib/tree/main/item/nnco/pl.py), is a custom module provided in the `nnco_lib` library. Given an input vector, `PLHead` feeds the vector to a [`LinearLayer`](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html), then the output of this liner layer is used to parametrize a PL distribution. Finally, the PL distribution is samped and the obtained solutions together with their log prababilities are returned."
      ],
      "metadata": {
        "id": "F8WcUxSdkCAL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = nn.Sequential(\n",
        "            ## for example, you can uncomment the two lines below to add an \n",
        "            ## extra layer to the generative model\n",
        "\n",
        "            # torch.nn.Linear(noise_dim, noise_dim),\n",
        "            # torch.nn.ReLU(),\n",
        "            \n",
        "            PLHead(\n",
        "                input_dim=noise_dim,\n",
        "                sample_length=size,\n",
        "                num_samples=num_samples,\n",
        "                device=device\n",
        "            ),\n",
        "        ).to(device)\n",
        "model"
      ],
      "metadata": {
        "id": "QyrWwL-NkGx6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to update the parameters of the generative model that we've just created, we'll initialize a PyTorch optimizer. \n",
        "In this case, the gradient descent algorithm employed is the [Adam](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html) algorithm (same as in the paper), but [other algorithms](https://pytorch.org/docs/stable/optim.html) can be used too."
      ],
      "metadata": {
        "id": "0tE21K90XKwE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = Adam(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "DMSKexuZkcuX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Main loop"
      ],
      "metadata": {
        "id": "uJ4WUs1_ku57"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First of all, let's create a simple function that will help us track the progress of the optimization process. This function just saves the best objective value found so far and prints some relevant information, such as the mean fitness of the current samples and the value of the loss function."
      ],
      "metadata": {
        "id": "HGeTPbW1mXBV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_fitness = []\n",
        "\n",
        "def logger(fitness_lst, iter, max_iters, loss, is_minim=False):\n",
        "    val_max = fitness_lst.max().item()\n",
        "    if len(best_fitness) == 0 or (val_max > best_fitness[-1] and not is_minim) or (val_max < best_fitness[-1] and is_minim):\n",
        "        best_fitness.append(val_max)\n",
        "    else:\n",
        "        best_fitness.append(best_fitness[-1])\n",
        "    print(f'{iter}/{max_iters} loss: {loss.item()}, mean: {fitness.mean()}, \\\n",
        "best: {best_fitness[-1]}')\n"
      ],
      "metadata": {
        "id": "_F4-JiOBmTUG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, the code bock below holds the main loop of the algorithm. Run it to start optimizing! âš¡\n",
        "\n",
        "This is the description of the steps that compose the main loop:\n",
        "\n",
        "1. Sample a set $\\mathcal{Z}$ of input vectors $\\mathbf{z}$ from a normal distribution:\n",
        "$$\\mathcal{Z} = \\{ \\mathbf{z}^i | \\mathbf{z}^i_j \\sim \\mathcal{N}(0, 1), j=1,\\ldots,m\\}_{i=1,\\ldots,\\tau}$$\n",
        "\n",
        "2. Sample a set of solutions $\\mathcal{D}^{\\mathbf{z}}$ for each input vector $\\mathbf{z}$, \n",
        "$$~{\\mathcal{D}^{\\mathbf z} = \\{ \\mathbf{x}^{i, \\mathbf z} | \\mathbf{x}^i \\sim P_\\theta(\\mathbf x | \\mathbf z) \\}_{i=1,\\ldots,\\lambda}}$$\n",
        "\n",
        "3. Compute the objective function value $f(\\mathbf{x})$ of each solution $\\mathbf{x}$.\n",
        "\n",
        "4. Standarize the objective function values obtained in the previous step.\n",
        "\n",
        "5. Compute the loss function value and the gradient estimate $d_t$:\n",
        "$$\n",
        "d_t = \\frac{1}{\\tau\\lambda} \\sum_{\\mathbf z \\in \\mathcal{Z}} \\sum_{\\mathbf x \\in \\mathcal{D}^{\\mathbf z}}\\nabla_\\theta \\log(P_\\theta (\\mathbf x| \\mathbf z))f(\\mathbf x)\n",
        "$$\n",
        "<br>And update the parameters $\\theta$ of the generative model $g_\\theta$ as, $$\\theta_{t+1} \\gets \\theta_t - \\alpha d_t$$\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "P9AHX_0Kmb8M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for iter in range(num_iters):\n",
        "    # (1)\n",
        "    z = torch.normal(mean=0, std=1, \n",
        "                     size=(batch_size, noise_dim), device=device)\n",
        "    \n",
        "    # (2)\n",
        "    samples, logps = model(z)\n",
        "\n",
        "    # (3)\n",
        "    fitness = [problem.evaluate(batch) for batch in samples.cpu().numpy()]\n",
        "    fitness = torch.as_tensor(fitness, dtype=torch.float32, device=device)\n",
        "\n",
        "    # (4)\n",
        "    u = utility.standarized_utility(fitness)\n",
        "\n",
        "    # (5)\n",
        "    loss = -(logps * u).mean()\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    logger(fitness, iter, num_iters, loss)\n",
        "\n",
        "print(f'Best objective function value: {best_fitness[-1]}')"
      ],
      "metadata": {
        "id": "tP03pBK_kxju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Result visualization"
      ],
      "metadata": {
        "id": "qdUCTo9UlK3B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(range(len(best_fitness)), best_fitness)\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Best objective function value')\n",
        "plt.title(instance.split('/')[-1])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WUcr9HjHlKZ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optimizing a PFSP instance\n",
        "\n",
        "The flexibility of the presented framework allows to optimize different combinatorial problems with minimal changes to the implementation. To illustrate this fact, in this section we'll approach a completly different CO problem, the PFSP. \n",
        "\n",
        "<br>"
      ],
      "metadata": {
        "id": "fDqJnwZw23a4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As it was the case with the LOP, the first step will be to import the optimization instance that we want to optimize. Note that the new instance to optimize has the same size as the one in the previous example, $n=50$, thus there is no need to compute the maximum number of iterations again."
      ],
      "metadata": {
        "id": "AWzsfbG84XMb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "instance = \"all_instances/PFSP/tai50_5_8.fsp\"\n",
        "problem = problems.pfsp.Pfsp(instance)"
      ],
      "metadata": {
        "id": "9n11KmLT4GO_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The generative model\n",
        "\n",
        "When optimizing the LOP, the employed probability distribution was the *Placket-Luce* distribution. \n",
        "With the purpose following the decissions made in the paper, we'll move to the *Univariate Marginals Distribution* (UMD) to optimize the PFSP.\n",
        "\n",
        "In consequence, the architecture of the generative model must change to output the parameters of the UMD instead of the PL distribution.\n",
        "\n",
        "> Again, the last layer of the model is a custom layer, [`UMDHead`](https://git.sr.ht/~mikelma/nnco_lib/tree/main/item/nnco/umd.py), implemented in `nnco_lib`. This layer is follows the same procedure as the `PLHead` model, except that it builts and samples a UMD instead of a PL distribution.\n",
        "\n",
        "> [`LinearParallel`](https://git.sr.ht/~mikelma/nnco_lib/tree/main/item/nnco/umd.py) is a custom layer that given an input vector(s), runs the input through $n$ linear layers, returning the generated $n$ output vectors."
      ],
      "metadata": {
        "id": "XUghdlVu5k1i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nnco.umd import UMDHead, LinearParallel\n",
        "model = nn.Sequential(\n",
        "            ## it is possible to replace the `LinearParallel` layer with a \n",
        "            ## normal Linear layer.\n",
        "\n",
        "            # torch.nn.Linear(noise_dim, noise_dim),\n",
        "            # torch.nn.ReLU(),\n",
        "\n",
        "            LinearParallel(\n",
        "                in_dim=noise_dim,\n",
        "                out_dim=noise_dim,\n",
        "                num_linears=size,\n",
        "                activation=nn.ReLU()),\n",
        "            UMDHead(\n",
        "                input_dim=noise_dim,\n",
        "                sample_length=size,\n",
        "                num_samples=num_samples,\n",
        "            )\n",
        "        ).to(device)\n",
        "\n",
        "# generate an optimizer for the new model\n",
        "optimizer = Adam(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "UJZTzIST5pb0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The main loop\n",
        "\n",
        "Before starting to optimize the PFSP instance, there are two more changes to the implementation shown in the LOP example:\n",
        "\n",
        "1. Instead of directly evaluating the permutations sampled from the probability distribution (see 3rd step in the block below), in the PFSP we'll evaluate the inverse of the samples (refer to the paper for more information). In summary, instead of calculating $f(\\sigma)$, $f(\\sigma^{-1})$ is computed.\n",
        "\n",
        "2. In the LOP example, there was a minus sign in front of the loss function (see 4th step). For the PFSP the minus sign has been removed. The change is caused by the fact thet the LOP is a maximization problem and the PFSP is a minimization problem (again, refer to the paper for additional information). "
      ],
      "metadata": {
        "id": "bZtjeffrE41S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pypermu import utils as permutils\n",
        "\n",
        "best_fitness = [] # restart best fitness list (used by `logger`)\n",
        "for iter in range(num_iters):\n",
        "    # (1)\n",
        "    z = torch.normal(mean=0, std=1, \n",
        "                     size=(batch_size, noise_dim), device=device)\n",
        "    \n",
        "    # (2)\n",
        "    samples, logps = model(z)\n",
        "\n",
        "    # (3)\n",
        "    permus = [permutils.transformations.marina2permu(b) for b in samples.cpu().numpy()]\n",
        "    permus = [permutils.transformations.permu2inverse(batch) for batch in permus]\n",
        "\n",
        "    fitness = [problem.evaluate(batch) for batch in permus]\n",
        "    fitness = torch.as_tensor(fitness, dtype=torch.float32, device=device)\n",
        "\n",
        "    # (4)\n",
        "    u = utility.standarized_utility(fitness)\n",
        "\n",
        "    # (5)\n",
        "    loss = (logps * u).mean()\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    logger(fitness, iter, num_iters, loss, is_minim=True)\n",
        "\n",
        "print(f'Best objective function value: {best_fitness[-1]}')"
      ],
      "metadata": {
        "id": "FhQurx-a6VdU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualizing the results"
      ],
      "metadata": {
        "id": "HbsRZeK4GrvC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(range(len(best_fitness)), best_fitness)\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Best objective function value')\n",
        "plt.title(instance.split('/')[-1])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9lspbB-b-yty"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}